<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data mining | Liz McConnell</title>
    <link>/tag/data-mining/</link>
      <atom:link href="/tag/data-mining/index.xml" rel="self" type="application/rss+xml" />
    <description>data mining</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Liz McConnell © 2020</copyright><lastBuildDate>Thu, 13 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu06d963f22cc5d003786a3f474238bf81_14484_512x512_fill_lanczos_center_2.png</url>
      <title>data mining</title>
      <link>/tag/data-mining/</link>
    </image>
    
    <item>
      <title>Making Maps with Leaflet in R</title>
      <link>/post/leaflet/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/post/leaflet/</guid>
      <description>


&lt;p&gt;I’ve been searching for a way to interactively display some geospatial data. After trying out a few things I found {leaflet} - so easy and beautiful! I started out reading this &lt;a href=&#34;https://rstudio.github.io/leaflet/&#34;&gt;excellent primer&lt;/a&gt;, then got to work with specific examples. Let’s take a look and see what it can do -&amp;gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#libraries
library(tidyverse) #for manipulating and visualizing data
library(leaflet) #for making interactive maps

#upload data from Geotracker - LA County
URL &amp;lt;- &amp;quot;https://geotracker.waterboards.ca.gov/data_download/geo_by_county/LosAngelesGeoXY.zip&amp;quot;
download.file(URL, destfile=&amp;#39;LosAngelesGeoXY.zip&amp;#39;, method=&amp;#39;curl&amp;#39;)
unzip(&amp;#39;LosAngelesGeoXY.zip&amp;#39;)

LA_XY &amp;lt;- read.delim(&amp;quot;LosAngelesGeoXY.txt&amp;quot;)
head(LA_XY)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        COUNTY   GLOBAL_ID FIELD_PT_NAME FIELD_PT_CLASS XY_SURVEY_DATE LATITUDE
## 1 Los Angeles T0603701211      GMX-RPZ7             PZ     2001-01-25 34.02664
## 2 Los Angeles T0603701211      GMX-RPZ8             PZ     2001-07-11 34.01503
## 3 Los Angeles T0603701211      GMX-RPZ9             PZ     2001-07-11 34.01506
## 4 Los Angeles T0603701211       KMX-MW1             MW     1999-08-27 34.01560
## 5 Los Angeles T0603701211       KMX-MW2             MW     1999-08-27 34.01787
## 6 Los Angeles T0603701211       KMX-MW3             MW     1999-08-27 34.01728
##   LONGITUDE XY_METHOD XY_DATUM XY_ACC_VAL  XY_SURVEY_ORG GPS_EQUIP_TYPE
## 1 -118.4214      CGPS    NAD83          3 Calvada Survey           L530
## 2 -118.4168      CGPS    NAD83          3 Calvada Survey           L530
## 3 -118.4169      CGPS    NAD83          3 Calvada Survey           L530
## 4 -118.4266      CGPS    NAD83          3 Calvada Survey           L530
## 5 -118.4251      CGPS    NAD83          3 Calvada Survey           L530
## 6 -118.4246      CGPS    NAD83          3 Calvada Survey           L530
##   XY_SURVEY_DESC
## 1               
## 2               
## 3               
## 4               
## 5               
## 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similar to the well measurements data, there is &lt;a href=&#34;https://www.waterboards.ca.gov/ust/electronic_submittal/docs/geotrackersurvey_xyz_4_14_05.pdf&#34;&gt;documentation&lt;/a&gt; on the geotracker for each of these column ids. The &lt;code&gt;GLOBAL_ID&lt;/code&gt; represents a site and matches up with the concentration data files. Let’s take a closer look at the &lt;code&gt;FIELD_PT_CLASS&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LA_XY %&amp;gt;%
  select(FIELD_PT_CLASS) %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  FIELD_PT_CLASS
##    :  1        
##  BH: 11        
##  MW:109        
##  PZ:  6        
##  SG: 30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are four codes in our data - BH, MW, PZ, SG. You might guess that BH is borehole and MW is monitoring well. I would guess that PZ stands for piezometer and I’m actually not sure what SG stands for. The documentation lists 33 valid values for &lt;code&gt;FIELD_PT_CLASS&lt;/code&gt;, but PZ and SG are not on the list.&lt;/p&gt;
&lt;p&gt;The date on the documentation is 2005, so maybe these codes have been added in the 15 years since then. A disclaimer at the top of the list states that new values are added occasionally, but the link to see the current list is broken :(&lt;/p&gt;
&lt;p&gt;In any case, we’ll plot these points and see what we can add to make them useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LA_wells_map &amp;lt;- leaflet(LA_XY) %&amp;gt;%
  addProviderTiles(&amp;quot;CartoDB.Positron&amp;quot;) %&amp;gt;% #using the CartoDB.Positron tiles; there are other options!
  addCircleMarkers(lng = ~LONGITUDE,
                   lat = ~LATITUDE, 
                   )
#LA_wells_map &lt;/code&gt;&lt;/pre&gt;
&lt;iframe seamless src=&#34;LA_wells_map.html&#34; width=&#34;100%&#34; height=&#34;500&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;Look at that! With just a few lines of code we’ve made a map of points that we can pan and zoom. I was honestly expecting a lot more sites. There are other basemaps that you can use, so take a look at &lt;a href=&#34;https://rstudio.github.io/leaflet/basemaps.html&#34;&gt;what’s available&lt;/a&gt; and pick what’s right for you. At this point my map doesn’t display that much information though, so let’s see what we can improve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#make a palette to add colors
pal &amp;lt;- colorFactor(topo.colors(5), LA_XY$FIELD_PT_CLASS)

LA_wells_map_with_color &amp;lt;- leaflet(LA_XY) %&amp;gt;%
  addProviderTiles(&amp;quot;CartoDB.Positron&amp;quot;) %&amp;gt;%
  addCircleMarkers(lng = ~LONGITUDE,
                   lat = ~LATITUDE, 
                   label= ~as.character(FIELD_PT_CLASS), #add a label
                   color = ~pal(FIELD_PT_CLASS)) #add colors

#LA_wells_map_with_color&lt;/code&gt;&lt;/pre&gt;
&lt;iframe seamless src=&#34;LA_wells_map_with_color.html&#34; width=&#34;100%&#34; height=&#34;500&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;This shows which class the points are and when you hover over the class it shows the class code. If you zoom in to the site near San Gabriel, you can see three monitoring wells (the two close to each other are probably the same one) and a pattern of &lt;code&gt;SG&lt;/code&gt; class points that is fairly uniform. The site near El Monte is made up only of boreholes, as is the one near Manhattan Beach. The others have a mix of monitoring wells and piezometers.&lt;/p&gt;
&lt;p&gt;The label field in our call to leaflet creates a tag that shows up when we hover our mouse over it. There’s another field for popup, which will make a box that appears when you click on a point. Let’s add some data and put it in a popup box. To do this we’ll have to get the concentration data for the sites and join by the &lt;code&gt;FIELD_PT_NAME&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;URL &amp;lt;- &amp;quot;https://geotracker.waterboards.ca.gov/data_download/edf_by_county/LosAngelesEDF.zip&amp;quot;
download.file(URL, destfile=&amp;#39;LosAngelesEDF.zip&amp;#39;, method=&amp;#39;curl&amp;#39;)
unzip(&amp;#39;LosAngelesEDF.zip&amp;#39;)

LA_EDF &amp;lt;- read.delim(&amp;quot;LosAngelesEDF.txt&amp;quot;)
str(LA_EDF)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    128666 obs. of  23 variables:
##  $ COUNTY       : Factor w/ 2 levels &amp;quot;\032&amp;quot;,&amp;quot;Los Angeles&amp;quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ GLOBAL_ID    : Factor w/ 8 levels &amp;quot;&amp;quot;,&amp;quot;SL603799209&amp;quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
##  $ FIELD_PT_NAME: Factor w/ 127 levels &amp;quot;&amp;quot;,&amp;quot;15200&amp;quot;,&amp;quot;15210&amp;quot;,..: 50 31 40 50 45 39 39 39 32 40 ...
##  $ LOGDATE      : Factor w/ 274 levels &amp;quot;&amp;quot;,&amp;quot;2006-07-10&amp;quot;,..: 56 46 46 56 47 46 46 46 46 46 ...
##  $ LOGTIME      : int  545 1230 945 850 1130 730 850 715 1215 900 ...
##  $ LOGCODE      : Factor w/ 10 levels &amp;quot;&amp;quot;,&amp;quot;AEII&amp;quot;,&amp;quot;EAIH&amp;quot;,..: 4 4 4 4 4 4 4 4 4 4 ...
##  $ SAMPID       : Factor w/ 2537 levels &amp;quot;&amp;quot;,&amp;quot;15200&amp;quot;,&amp;quot;15210&amp;quot;,..: 2062 1835 1954 2061 2033 1934 1935 1936 1852 1952 ...
##  $ MATRIX       : Factor w/ 7 levels &amp;quot;&amp;quot;,&amp;quot;AX&amp;quot;,&amp;quot;GS&amp;quot;,&amp;quot;IA&amp;quot;,..: 6 6 6 6 6 6 6 6 6 6 ...
##  $ LABWO        : Factor w/ 45 levels &amp;quot;&amp;quot;,&amp;quot;1609-03&amp;quot;,&amp;quot;1803292&amp;quot;,..: 45 45 45 45 45 45 45 45 45 45 ...
##  $ LABCODE      : Factor w/ 9 levels &amp;quot;&amp;quot;,&amp;quot;AAC&amp;quot;,&amp;quot;ASLL&amp;quot;,..: 5 5 5 5 5 5 5 5 5 5 ...
##  $ LABSAMPID    : Factor w/ 2656 levels &amp;quot;&amp;quot;,&amp;quot;060703521&amp;quot;,..: 525 437 443 527 455 439 440 435 436 442 ...
##  $ ANMCODE      : Factor w/ 24 levels &amp;quot;&amp;quot;,&amp;quot;8260+OX&amp;quot;,&amp;quot;8260FA&amp;quot;,..: 23 23 23 23 23 23 23 23 23 23 ...
##  $ LABLOTCTL    : Factor w/ 652 levels &amp;quot;&amp;quot;,&amp;quot;060711B01&amp;quot;,..: 144 123 127 144 129 123 123 123 123 123 ...
##  $ ANADATE      : Factor w/ 384 levels &amp;quot;&amp;quot;,&amp;quot;2006-07-11&amp;quot;,..: 87 72 74 87 76 72 72 72 72 72 ...
##  $ BASIS        : Factor w/ 7 levels &amp;quot;&amp;quot;,&amp;quot;A&amp;quot;,&amp;quot;D&amp;quot;,&amp;quot;L&amp;quot;,..: 5 5 5 5 5 5 5 5 5 5 ...
##  $ PARLABEL     : Factor w/ 150 levels &amp;quot;&amp;quot;,&amp;quot;4:2FTS&amp;quot;,&amp;quot;6:2FTS&amp;quot;,..: 80 12 56 141 90 141 100 82 141 68 ...
##  $ PARVAL       : num  10 1 1 1 10 1 5 1 1 2 ...
##  $ PARVQ        : Factor w/ 5 levels &amp;quot;&amp;quot;,&amp;quot;&amp;lt;&amp;quot;,&amp;quot;=&amp;quot;,&amp;quot;ND&amp;quot;,..: 2 2 2 2 3 2 2 2 2 2 ...
##  $ LABDL        : num  5.4 0.24 0.36 0.23 4.3 0.23 1.5 0.26 0.23 0.18 ...
##  $ REPDL        : num  10 1 1 1 10 1 5 1 1 2 ...
##  $ UNITS        : Factor w/ 10 levels &amp;quot;&amp;quot;,&amp;quot;MG/KG&amp;quot;,&amp;quot;MG/L&amp;quot;,..: 8 8 8 8 8 8 8 8 8 8 ...
##  $ DILFAC       : num  1 1 1 1 1 1 5 1 1 1 ...
##  $ LNOTE        : Factor w/ 72 levels &amp;quot;&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;B,VCO&amp;quot;,..: 1 1 1 1 68 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ooof that’s a lot of data. The question now is what do we want to put in the popup? Most recent concentration of a small class of chemicals? Maximum concentration? Could we add a plot of the concentration over time? I haven’t been able to get the plot-within-a-plot working (yet), so let’s go with maximum concentration of Trichloroethene (TCE) at each well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TCE_conc &amp;lt;- LA_EDF %&amp;gt;%
  filter(PARLABEL == &amp;quot;TCE&amp;quot;) %&amp;gt;% 
  group_by(FIELD_PT_NAME) %&amp;gt;% 
  mutate(TCE = max(PARVAL)) %&amp;gt;%
  select(FIELD_PT_NAME, TCE, UNITS) %&amp;gt;%
  filter(UNITS %in% c(&amp;quot;UG/L&amp;quot;, &amp;quot;MG/L&amp;quot;)) %&amp;gt;%
  unique() %&amp;gt;%
  mutate(value = case_when(UNITS == &amp;quot;MG/L&amp;quot; ~ 1000, T ~ 1)) %&amp;gt;% #create a col with conversion factor
  mutate(TCE = TCE*value) #convert mg/L to ug/L

LA_TCE_xy &amp;lt;- TCE_conc %&amp;gt;%
  inner_join(LA_XY, by = &amp;quot;FIELD_PT_NAME&amp;quot;) %&amp;gt;%
  select(FIELD_PT_NAME, TCE, LATITUDE, LONGITUDE)

TCEpal &amp;lt;- colorNumeric(
  palette = &amp;quot;YlGnBu&amp;quot;,
  domain = 0:110)

popup1 &amp;lt;- &amp;quot;TCE concentration: &amp;quot;  #I bet there&amp;#39;s a better way to do this
popup2 &amp;lt;- &amp;quot; ug/L&amp;quot;                #Tell me in the comments!

LA_TCE_xy$pop1 &amp;lt;- popup1         #Like and subscribe!
LA_TCE_xy$pop2 &amp;lt;- popup2
LA_TCE_xy$popup &amp;lt;- paste0(LA_TCE_xy$pop1, LA_TCE_xy$TCE, LA_TCE_xy$pop2)

LA_wells_TCE_map &amp;lt;- leaflet(LA_TCE_xy) %&amp;gt;%
  addProviderTiles(&amp;quot;CartoDB.Positron&amp;quot;) %&amp;gt;%
  addCircleMarkers(lng = ~LONGITUDE,
                   lat = ~LATITUDE, 
                   radius=8, 
                   stroke=FALSE,
                   fillOpacity = 0.2, 
                   color = ~TCEpal(TCE),
                   popup = ~popup) %&amp;gt;% #add colors
  addLegend(pal = TCEpal, values = ~TCE, opacity = 0.7, title = NULL,
  position = &amp;quot;bottomright&amp;quot;)

#LA_wells_TCE_map&lt;/code&gt;&lt;/pre&gt;
&lt;iframe seamless src=&#34;LA_wells_TCE_map.html&#34; width=&#34;100%&#34; height=&#34;500&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You may notice some annoying things about this map; most of the values are very low so they skew the colors to almost zero and the scale starts with highest values on the bottom, which is counterintuitive to me. Also we’ve lost some points - the spatial file had 157 points but when we join it with concentration data we’re left with 69 points.&lt;/p&gt;
&lt;p&gt;One common way to visualize data with a large range is with a log transformation. Let’s try it out!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LA_TCE_xy &amp;lt;- LA_TCE_xy %&amp;gt;%        #log transform the concentrations
  mutate(TCE = case_when(TCE == 0 ~ 0.0001, T ~ TCE)) %&amp;gt;%
  mutate(log_TCE = log(TCE))

logpal &amp;lt;- colorNumeric(
  palette = &amp;quot;YlGnBu&amp;quot;,
  domain = -9.2105:4.701)

LA_wells_logTCE_map &amp;lt;- leaflet(LA_TCE_xy) %&amp;gt;%
  addProviderTiles(&amp;quot;CartoDB.Positron&amp;quot;) %&amp;gt;%
  addCircleMarkers(lng = ~LONGITUDE,
                   lat = ~LATITUDE, 
                   radius=8, 
                   stroke=FALSE,
                   fillOpacity = 0.2, 
                   color = ~logpal(log_TCE),
                   popup = ~popup) 

#LA_wells_logTCE_map&lt;/code&gt;&lt;/pre&gt;
&lt;iframe seamless src=&#34;LA_wells_logTCE_map.html&#34; width=&#34;100%&#34; height=&#34;500&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;If we use the log of the TCE value it’s easier to see the changes in color., though it could still use some fiddling with. I took out the scale becuse the popup still says the real value but the color is now based on a log value. There are probably better color schemes out there but that’s like entering another world, so I’ll leave it at this.&lt;/p&gt;
&lt;p&gt;I’m sure that there are many ways to improve this map and build on these concepts in leaflet. Take a look at what’s out there an let me know if you discover anything you love!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>USGS Data Retrieval</title>
      <link>/post/streamflowdata/</link>
      <pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/streamflowdata/</guid>
      <description>


&lt;p&gt;It took me a few times to get this right - the interface of the USGS website is a rabbit hole of buttons and options for data retrieval. It’s not so hard to get the data you’re interested in, but pay close attention because it’s easy to do the wrong thing. Probably the easiest way is to use the &lt;code&gt;dataRetrieval&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html&#34;&gt;package&lt;/a&gt;. The last commits on their &lt;a href=&#34;https://github.com/USGS-R/dataRetrieval&#34;&gt;github&lt;/a&gt; are from 2 months ago, so it’s pretty up-to-date.&lt;/p&gt;
&lt;p&gt;To use this package, you need to know a few things about the structure of the data. Each USGS streamflow station has a number - you’ll need to know the number for the station you want. There’s also a number associated with each measured value; &lt;code&gt;00060&lt;/code&gt; for discharge in feet per second, &lt;code&gt;00065&lt;/code&gt; for gauge height in feet, &lt;code&gt;00010&lt;/code&gt; for temperature in degrees celcius, and many more. The last critical thing to know is the date range of the data you’re interested in.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Station Number&lt;/li&gt;
&lt;li&gt;Parameter Code&lt;/li&gt;
&lt;li&gt;Date Range of Interest&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s an example from the documentation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install.packages(&amp;quot;dataRetrieval&amp;quot;) #uncomment to install the package
library(dataRetrieval)
# Choptank River near Greensboro, MD:
siteNumber &amp;lt;- &amp;quot;01491000&amp;quot;
parameterCd &amp;lt;- &amp;quot;00060&amp;quot;  # Discharge
startDate &amp;lt;- &amp;quot;2009-10-01&amp;quot;  
endDate &amp;lt;- &amp;quot;2012-09-30&amp;quot; 

discharge &amp;lt;- readNWISdv(siteNumber, 
                    parameterCd, startDate, endDate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what the resulting data frame looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;discharge.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fifth column is a quality code - A stands for approved for release by the USGS.&lt;/p&gt;
&lt;p&gt;Another notable argument to the &lt;code&gt;readNWISdv&lt;/code&gt; function is &lt;code&gt;statCd&lt;/code&gt;. This allows you to request daily data and specify the statistic used. For example, you can request the daily maximum, mean, median, or total.&lt;/p&gt;
&lt;p&gt;Pretty simple, right?&lt;/p&gt;
&lt;p&gt;There are some other handy functions and options in this library. You’ll notice that the colunm names aren’t really descriptive of the data. You can use another function from the package to rename them. Try this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;discharge &amp;lt;- renameNWISColumns(discharge)
names(discharge)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;agency_cd&amp;quot; &amp;quot;site_no&amp;quot;   &amp;quot;Date&amp;quot;      &amp;quot;Flow&amp;quot;      &amp;quot;Flow_cd&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That can clean things up a lot - especially if you’re importing many parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ggplot(discharge, aes(x=Date, y=Flow)) + geom_point(alpha = 0.4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/streamflowdata/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What I am interested is the streamflow data, so this is as far as I need to go. You can also import different kinds of USGS data using other functions of the same package. It’s a treasure trove! Check out the &lt;a href=&#34;https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html&#34;&gt;documentation&lt;/a&gt; to get the full scope of its powers.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I also came across another package called &lt;code&gt;waterData&lt;/code&gt; that can do the same retrieval and maybe some more analysis on the data. I haven’t looked further into this, but the last commits to their &lt;a href=&#34;https://github.com/USGS-R/waterData&#34;&gt;github&lt;/a&gt; are from 3 years ago, so I choose the &lt;code&gt;dataRetrieval&lt;/code&gt; package.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Now I’ll let you in on the less fun bit, which is the way I originally tried to download the data locally then upload to R. I started out fumbling around the website trying to find the gauge closest to the site I’m interested in. There’s an interactive map on the &lt;a href=&#34;https://waterdata.usgs.gov/nwis/rt&#34;&gt;homepage&lt;/a&gt;; you click on a state to get to a more refined map and click around some more to find the gauge you want.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;map.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You get to a really promising page that gives you some options to select parameters, a date range, and whether you want the output to be a graph, table, or tab-seperated format. This looks good, but is not.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;options.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I chose the tab-seperated option; After a lengthy wait a new tab with the output opened in the browser - so no file download. I guess I could have copied and pasted all of that into a text file, but I decided to search for a way to actually download the file.&lt;/p&gt;
&lt;p&gt;I found a &lt;a href=&#34;https://help.waterdata.usgs.gov/tutorials/overview/a-primer-on-downloading-data&#34;&gt;USGS primer on dowloading data&lt;/a&gt; (always read the instructions first!), which took some different turns than the path I took previoulsy. The user interface is difficult to navigate and not intuitive, but it’s a great resource for data. I won’t go through the whole process, because it’s mostly just clicking on the right spot and choosing your desired options, but here is the most important part:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;savetofile.png&#34; /&gt;
There are still extra steps of choosing the options in the website, downloading the data, then reading it into R. This is the code I used to read the data once I had downloaded it as a text file and renamed it “streamflow”. If I were to go back, I would &lt;em&gt;not&lt;/em&gt; do it this way, but I guess it doesn’t require downloading a new package and is fine if you’re not dowloading a lot of data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

streamflow &amp;lt;- read.table(&amp;quot;streamflow&amp;quot;, skip = 32) #the first 32 lines are metadata

colnames(streamflow) &amp;lt;- c(&amp;quot;agency&amp;quot;, &amp;quot;site_no&amp;quot;, &amp;quot;date&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;timezone&amp;quot;, &amp;quot;Flow&amp;quot;, &amp;quot;quality_code&amp;quot;)
#discharge is in cubic feet per second, quality code A means approved 

streamflow$date &amp;lt;- as.Date(streamflow$date)

ggplot(streamflow, aes(x=date, y=Flow)) + geom_point(alpha=0.4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/streamflowdata/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hope this info helps you avoid wasting time like I did on my first try! Do you know of a better way to get the data into R? Did you encounter any pitfalls?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Historical Weather Data with Dark Sky API</title>
      <link>/post/weather/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/weather/</guid>
      <description>


&lt;p&gt;In my IoT class last year with &lt;a href=&#34;https://soilcrop.agsci.colostate.edu/faculty-2/ham-jay/&#34;&gt;Jay Ham&lt;/a&gt; we used a website called &lt;a href=&#34;https://darksky.net/&#34;&gt;Dark Sky&lt;/a&gt; to get current weather conditions. I’ve been thinking about this recently, since I would like to see if I can match up weather conditions to the changes in the depth to water of wells at a site. I was inspired to look into this based on a talk from Jonathan Kennel from the Univesity of Guelph ( &lt;em&gt;Happy Canada Day!&lt;/em&gt; ) and several conversations with my advisor.&lt;/p&gt;
&lt;p&gt;I’ll walk through how I imported the data to R.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Dark Sky is a great resource, however, when I went to re-visit the website I found that they have joined Apple. This means they are no longer creating new accounts. Luckily I already had one from my class last fall. The API is still supported through at least the end of 2021. Later I’ll mention some ways that you could get similar (maybe better) data through other channels.&lt;/p&gt;
&lt;p&gt;The API allows up to 1000 calls per day for free. Using the &lt;em&gt;Time Machine&lt;/em&gt; you can request data from a certain date and location. I focused on hourly data, though it’s probably finer resolution than I need.&lt;/p&gt;
&lt;p&gt;“The &lt;code&gt;hourly&lt;/code&gt; data block will contain data points starting at midnight (local time) of the day requested, and continuing until midnight (local time) of the following day.”&lt;/p&gt;
&lt;p&gt;The docs include a sample API call, which includes your key, the location, and the date requested.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;GET https://api.darksky.net/forecast/0123456789abcdef9876543210fedcba/42.3601,-71.0589,255657600?exclude=currently,flags&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;A quick visit to my best friend stack overflow provided &lt;a href=&#34;https://stackoverflow.com/questions/46069322/r-api-call-for-json-data-and-converting-to-dataframe&#34;&gt;a little more clarity&lt;/a&gt; about how to use the API in R. The date is in UNIX format. I wanted to start at January 1, 2000, so I used a handy &lt;a href=&#34;https://www.unixtimestamp.com/&#34;&gt;UNIX converter&lt;/a&gt; to find my desired date number, 946684800. I replaced the url and now I’m ready to call the API.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#GET the url 
req &amp;lt;- httr::GET(&amp;quot;https://api.darksky.net/forecast/{key}/30.012188,-94.024525,946684800?exclude=currently,flags&amp;quot;)
req$status_code 
# should be 200 if it worked. If you get 400, something has gone wrong.

# extract req$content 
cont &amp;lt;- req$content

#Convert to char
char &amp;lt;- rawToChar(req$content)

#Convert to df 
df &amp;lt;- jsonlite::fromJSON(char)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked! I removed my private API key, so you’ll have to take my word for it. But now I have a new problem - the call only works for one day at a time. I want a lot of days, so I decided to write a loop.&lt;/p&gt;
&lt;p&gt;One thing I couldn’t get to work was changing the date inside the string for the API url. I posted in the R for Data Science Slack, and a few minutes later I learned a handy new trick - you can put a variable inside a string by just inserting it with quotes around the variable. Something like this:
&lt;code&gt;&amp;quot;https://api.darksky.net/forecast/{key}/30.012188,-94.024525,&amp;quot;,day,&amp;quot;?exclude=currently,flags&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Great! I ran the loop and it worked, kinda. It errored out after the first run because rbind could not combine two data frames with different numbers of columns. After looking at the next few days to see which columns were off I saw that it went from 15 to 16 to 17, then back down. Very annoying! They must have added some new info for some days, but this made the data inconsistent so I had to add a select function to the loop. I selected for the 15 columns that were consistent across all days and ran it again. Success!&lt;/p&gt;
&lt;p&gt;Here’s the code I ended up with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

#initialize all_hours so there&amp;#39;s something to rbind to
df &amp;lt;- jsonlite::fromJSON(paste0(&amp;quot;https://api.darksky.net/forecast/{key}/30.012188,-94.024525,946684800?exclude=currently,flags&amp;quot;))
all_hours &amp;lt;- df$hourly$data

# make a vector of unix dates I want (minus the first one, which I already put in all_hours)
unix_day &amp;lt;- seq(946771200, 1033084800, by=86400) 

for (day in unix_day){
  
df &amp;lt;- jsonlite::fromJSON(paste0(&amp;quot;https://api.darksky.net/forecast/{key}/30.012188,-94.024525,&amp;quot;,
day,
&amp;quot;?exclude=currently,flags&amp;quot;))

hourly &amp;lt;- select(df$hourly$data, c(cols))
                 
all_hours &amp;lt;- rbind(hourly, all_hours)}

#convert unix time to date
all_hours$time &amp;lt;- as.POSIXct(all_hours$time, origin=&amp;quot;1970-01-01&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I selected the columns I want and saved them as weather.csv. Let’s zoom in to two rainy days in May 2000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
weather &amp;lt;- read.csv(&amp;quot;weather.csv&amp;quot;)
#weather &amp;lt;- weather[2810:2845,]

#convert unix time to date
weather$time &amp;lt;- as.POSIXct(weather$time, origin=&amp;quot;1970-01-01&amp;quot;)

ggplot(weather, aes(x = time, y = precipIntensity, group =1)) + 
  geom_point(alpha = 0.4, color = &amp;quot;blue&amp;quot;) +
  geom_line(alpha = 0.4, color = &amp;quot;blue&amp;quot;, size = 0.5) +
  theme_gray() +
  theme(axis.text.x=element_text(angle=90, hjust=1)) +
  labs(title =&amp;quot;Precipitation Over Time&amp;quot;, x = &amp;quot;Date&amp;quot;, y = &amp;quot;Precipitation in Inches&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/weather/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can clearly see the precipitation-generating storm events over a few months.&lt;/p&gt;
&lt;p&gt;It will take me a few days of using my 1000 free API calls to cover the period I’m interested in, but overall it was really easy.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;other-weather-options&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other Weather Options&lt;/h2&gt;
&lt;p&gt;Clearly getting historical weather data for a certain location can be really useful, but since Dark Sky is no longer creating accounts it’s not a very practical resource.&lt;/p&gt;
&lt;p&gt;The helpful commenters on the R for Data Science Slack also suggested using &lt;a href=&#34;https://www.ncdc.noaa.gov/data-access/land-based-station-data&#34;&gt;NOAA&lt;/a&gt; to get the data. This is probably a more robust dataset anyway, but the downside is that they don’t have the option to use lat/long as a location - you have to pick from one of their existing stations. In my case there wasn’t one near the field site I was interested in, but they’re spread out across the country so I bet it will be a great source for a lot of people. There’s a good tutorial on accessing this data at &lt;a href=&#34;https://ropensci.org/tutorials/rnoaa_tutorial/&#34;&gt;R Open Sci&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve also hear the &lt;a href=&#34;https://www.wunderground.com/pws/overview&#34;&gt;Weather Underground&lt;/a&gt; has a good API, but it looks like you need to contribute weather data with an IoT device to access it. Cool! But may not be useful to some.&lt;/p&gt;
&lt;p&gt;Are there any other sources of weather data that you know of? Do you have suggestions to improve the approach I used for the Dark Sky data?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
