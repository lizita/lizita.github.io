<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>big data | Liz McConnell</title>
    <link>/tag/big-data/</link>
      <atom:link href="/tag/big-data/index.xml" rel="self" type="application/rss+xml" />
    <description>big data</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Liz McConnell © 2020</copyright><lastBuildDate>Sun, 25 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu06d963f22cc5d003786a3f474238bf81_14484_512x512_fill_lanczos_center_2.png</url>
      <title>big data</title>
      <link>/tag/big-data/</link>
    </image>
    
    <item>
      <title>Joining by Nearby Dates</title>
      <link>/post/fuzzyjoin/</link>
      <pubDate>Sun, 25 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/fuzzyjoin/</guid>
      <description>


&lt;p&gt;I’ve been wrestling with ideas for how to standardize sampling dates for a while now. The data sets I’m working with have fairly regular sampling intervals, but sometimes they aren’t sampled on the same day - and this drives me crazy. In terms of real-world sampling plans it makes total sense - weekends, rainy days, multi-day sampling events - but in terms of cleaning and working with the data it’s a pain. Here I’ve made up some fake data that shows the problem I’m having, then use fuzzyjoin to make it work. In the process I realized I could probably do approximately the same thing using dplyr. It’s a snowy day in Colorado, so let’s get to it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
well &amp;lt;- t(data.frame(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4)) #making some fake wells
date &amp;lt;- seq.Date(ymd(&amp;quot;1996-01-01&amp;quot;), ymd(&amp;quot;1996-12-01&amp;quot;), by = &amp;quot;quarter&amp;quot;) #making some fake dates
date &amp;lt;- rep(date, times = 4)
date &amp;lt;- date + sample(-7:7,16,replace=T)
concentration &amp;lt;- sample(1:20,16,replace=T) #making some fake concentrations
df_a &amp;lt;- cbind.data.frame(well, date, concentration) #first fake dataframe
df_a &amp;lt;- remove_rownames(df_a)

date &amp;lt;- date + sample(-7:7,16,replace=T)
concentration &amp;lt;- sample(1:20,16,replace=T)
df_b &amp;lt;- cbind.data.frame(well, date, concentration) #second fake dataframe
df_b &amp;lt;- remove_rownames(df_b)

df_a #one set of samples&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    well       date concentration
## 1     1 1996-01-07            13
## 2     1 1996-03-27             7
## 3     1 1996-07-02            20
## 4     1 1996-09-28            17
## 5     2 1995-12-29             5
## 6     2 1996-03-30            12
## 7     2 1996-06-28            19
## 8     2 1996-10-05            14
## 9     3 1995-12-30            19
## 10    3 1996-04-05            13
## 11    3 1996-07-04             8
## 12    3 1996-10-03             9
## 13    4 1996-01-03            11
## 14    4 1996-04-07            11
## 15    4 1996-06-30             4
## 16    4 1996-09-29             5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_b #another set of samples&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    well       date concentration
## 1     1 1996-01-06             7
## 2     1 1996-04-03             7
## 3     1 1996-06-30             4
## 4     1 1996-09-29            15
## 5     2 1995-12-30             1
## 6     2 1996-03-31            20
## 7     2 1996-07-02            16
## 8     2 1996-09-29            15
## 9     3 1995-12-24            10
## 10    3 1996-04-10            12
## 11    3 1996-07-03            12
## 12    3 1996-10-08             7
## 13    4 1996-01-08            11
## 14    4 1996-04-10             9
## 15    4 1996-06-30            10
## 16    4 1996-10-01            11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_ab &amp;lt;- rbind(df_a, df_b)
ggplot(df_ab, aes(x=date, y=concentration, color=well)) + geom_point(alpha=0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fuzzyjoin/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;
Do you see the problem?!?! I can’t make the neat dataframe with both observations that I want. If I join on exact dates I loose a bunch of data. After some googling I found a package called {fuzzyjoin} that allows you to join columns within a certain tolerance, so they don’t have to exactly match up. Great - that’s what I want. Unfortunately, in this case I want to join by two columns, &lt;code&gt;well&lt;/code&gt; and &lt;code&gt;date&lt;/code&gt;, but the &lt;code&gt;max_dist&lt;/code&gt; argument will apply to both, so since I just made the wells 1-4, if I wanted to capture dates within more than four days of each other all wells would be considered matches. So I fuzzy joined first by date, then filtered for the observations where well.x matches well.y. After some cleaning up, it looks pretty good.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fuzzyjoin)
df_c &amp;lt;- difference_inner_join(df_a, df_b, by = &amp;quot;date&amp;quot;, max_dist= 14) #join dates within two weeks
df_c &amp;lt;- df_c %&amp;gt;% 
  filter(well.x == well.y) %&amp;gt;% #keep only joined observations from the same well
  rename(&amp;quot;PCE&amp;quot;=concentration.x, &amp;quot;TCE&amp;quot;=concentration.y, &amp;quot;well&amp;quot;=well.x) %&amp;gt;% #change some names
  mutate(date = round_date(date.x, unit = &amp;quot;month&amp;quot;)) %&amp;gt;% #round to nearest month
  select(-c(&amp;quot;well.y&amp;quot;, &amp;quot;date.x&amp;quot;, &amp;quot;date.y&amp;quot;)) #remove duplicate well name

head(df_c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   well PCE TCE       date
## 1    1  13   7 1996-01-01
## 2    1   7   7 1996-04-01
## 3    1  20   4 1996-07-01
## 4    1  17  15 1996-10-01
## 5    2   5   1 1996-01-01
## 6    2  12  20 1996-04-01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_c %&amp;gt;% pivot_longer(cols = c(PCE, TCE)) %&amp;gt;%
  ggplot(aes(x=date, y=value, color=well)) + geom_point(alpha=0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fuzzyjoin/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
When I did the &lt;code&gt;round_date&lt;/code&gt; thing above I thought, well, couldn’t I just have rounded by date first, then joined by exact date after? So I did just that and got the same results. This wouldn’t always give the same results, for example if a well was sampled around the middle of the month and one observation was rounded up while another was rounded down. The {fuzzyjoin} method joins relative to the distance between the dates, not based on the rounded date. In most cases it won’t matter, but there’s a small distinction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_a &amp;lt;- mutate(df_a, date = round_date(date, unit = &amp;quot;month&amp;quot;)) 
df_b &amp;lt;- mutate(df_b, date = round_date(date, unit = &amp;quot;month&amp;quot;))
#df_c &amp;lt;- inner_join(df_a, df_b, by = date)
# ^this causes an error: Error: `by` must be a (named) character vector, list, or NULL, not a `Date` object.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_a$date &amp;lt;- as.double(df_a$date)
df_b$date &amp;lt;- as.double(df_b$date)

df_c &amp;lt;- inner_join(df_a, df_b, by = &amp;quot;date&amp;quot;)
df_c &amp;lt;- df_c %&amp;gt;% 
  filter(well.x==well.y) %&amp;gt;%
  rename(&amp;quot;PCE&amp;quot;=concentration.x, &amp;quot;TCE&amp;quot;=concentration.y, &amp;quot;well&amp;quot;=well.x) %&amp;gt;%
  select(-c(&amp;quot;well.y&amp;quot;)) #remove duplicate well name
df_c &amp;lt;- df_c %&amp;gt;%
  mutate(date = as.Date(date, origin = &amp;quot;1970-01-01&amp;quot;))
head(df_c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   well       date PCE TCE
## 1    1 1996-01-01  13   7
## 2    1 1996-04-01   7   7
## 3    1 1996-07-01  20   4
## 4    1 1996-10-01  17  15
## 5    2 1996-01-01   5   1
## 6    2 1996-04-01  12  20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_c %&amp;gt;% pivot_longer(cols = c(PCE, TCE)) %&amp;gt;%
  ggplot(aes(x=date, y=value, color=well)) + geom_point(alpha=0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fuzzyjoin/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
Now it’s time to apply this to a real dataset and see what problems I’ll run into…because they’re out there…lurking…spooky!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weapons of Math Destruction</title>
      <link>/post/wmds/</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/post/wmds/</guid>
      <description>


&lt;p&gt;Earlier this summer I got an email about reading groups for grad students hosted by the department of cell and molecular biology. Who wasn’t hungry for a reading group this summer? The first book I signed up to read was Weapons of Math Destruction by Cathy O’Neil. I recognized it because I had heard an &lt;a href=&#34;https://www.npr.org/2016/09/12/493654950/weapons-of-math-destruction-outlines-dangers-of-relying-on-data-analytics&#34;&gt;NPR interview&lt;/a&gt; with the author on the radio. I actually listened to the audiobook because I wasn’t in Fort Collins and libraries were closed (COVID). It was a really engaging story - basically the whole book is composed of examples of “Weapons of Math Destruction”, or WMDs, a term that O’Neil coins in the first chapted to describe models that are important, opaque, and reinforce inequality.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;The features of WMDs are highlighted through examples (so many examples!) of real algorithms that are in use every day. But O’Neil doesn’t just throw lots of examples of WMDs at us, she also describes models that have some negative aspects, but are not on the whole that harmful, and models that are sound uses of statistics. It’s complicated and forces you to confront how insidious these algorithms are.&lt;/p&gt;
&lt;p&gt;Something that surprised me was her discussion of credit scores. I was all ready to jump on board against credit scores, but that wasn’t her conclusion in the book. This is &lt;em&gt;not&lt;/em&gt; a WMD. The FICO model has a feedback loop, but it corrects itself rather than perpetuates inequality. If many people who are predicted to be risky borrowers pay on time, the model is updated to account for the new data and make better predictions. In addition, the model is fairly transparent - if you are trying to improve your credit score you can easily look up ways to do so. And credit scores are also regulated, so you have legal rights to see your score and know what goes into it.&lt;/p&gt;
&lt;p&gt;In stark contrast are ubiquitous e-scores used by credit card companies, car dealerships, and advertising agencies that you probably have no idea exist. These scores are created by models that combine lots of seemingly disparate information about you and predict if you’re likely to buy their product. The problem is that the factors the model uses are proxies like zip code, which means that inequality created by racist zoning laws is perpetuated by an algorithm. Predatory for-profit college lead generation companies use algorithms that target people who are earnestly searching for oppurtunities at upward mobility - and make a fortune doing it.&lt;/p&gt;
&lt;p&gt;Another outraging story was about scheduling software used by low-wage corporations like McDonald’s, Starbucks, and Walmart. They’re designed to optimize saving for the company, so they create ever-changing and erratic schedules that keep workers from qualifying for benefits. In 2014 the &lt;em&gt;New York Times&lt;/em&gt; published an article profiling Jannette Navarro, a Starbucks employee and single mother trying to work her way through college. Her life was so dominated by her work schedule that she couldn’t make her classes consistently or plan for any sort of normal day-care schedule. After the story, legislators drafted a bill to regulate scheduling software but it went nowhere.&lt;/p&gt;
&lt;p&gt;I’ll leave you with most shocking statistic I read, related to the insurance industry.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;In Florida, adults with clean driving records
and poor credit scores paid an average of $1,552
more than the same drivers with excellent credit
and a &lt;em&gt;drunk driving convition&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A seemingly inocuous number created by an algorithm is plugged in to another algorithm for an utterly ridiculous outcome.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;In the discussion group one person said they were vaguely aware that things like this were happening, but didn’t know the extent or specific examples. It does get somewhat repetitive since the author really pounds the ideas into you with tons of examples, but that didn’t bother me. Another person asked that now that we’ve read the book, what can we do about it? We didn’t come up with a lot of answers - be aware and support policies that regulate them. Are there other actions that people can take to combat the proliferation of Weapons of Math Destruction?&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
